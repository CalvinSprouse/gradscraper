{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "25895c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import csv\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import uuid\n",
    "import webbrowser\n",
    "import os\n",
    "from pathlib import Path\n",
    "from os import PathLike\n",
    "from typing import Iterable\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "import yaml\n",
    "from bs4 import BeautifulSoup\n",
    "from rich import print\n",
    "from selenium import webdriver\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "45925008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define save directory for csv files\n",
    "csv_dir = Path('csv_files')\n",
    "csv_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# define the masters program entry url for parsing\n",
    "masters_entry_url = r'https://www.mastersportal.com/search/master/msc/physics?de=fulltime&mh=face2face&sorting=tuition'\n",
    "\n",
    "# define the directory for obsidian\n",
    "obsidian_dir = Path(os.path.expanduser('~'), 'obsnotes', 'gradnotes')\n",
    "research_dir = obsidian_dir / 'options'\n",
    "\n",
    "# define new keys for the user to research, key:desc\n",
    "research_tasks = {\n",
    "    'uni-url': 'University homepage URL',\n",
    "    'dept-url': 'Department homepage URL',\n",
    "    'program-url': 'Program specific URL',\n",
    "    'funding-url': 'Funding information URL',\n",
    "    'scholarship-url': 'Additional funding sources URL',\n",
    "    # 'research-url': 'Homepage of department research URL',\n",
    "    'app-url': 'Link to the application page',\n",
    "    'app-fee': 'Application fee in USD$',\n",
    "    # 'tuition-est': 'Estimated cost of attendance in USD$/yr',\n",
    "    # 'funding-est': 'Estimated annual funding in USD$/yr',\n",
    "    # 'research-topic': 'Enter the most preferred research topic available',\n",
    "    # 'veto': 'True/False for automatic veto for any reason',\n",
    "    # 'completed': 'Mark True when completed analysis',\n",
    "}\n",
    "\n",
    "# describe some default document headers\n",
    "document_prompts = {\n",
    "    'Research Preferences': '*Describe potential research groups and topics*',\n",
    "    'Funding Practices': '*Describe the costs of living and potential funding sources*',\n",
    "    'Application Requirements': '*Describe application requirements*',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2639fcdd",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680de10",
   "metadata": {},
   "source": [
    "## Program Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4ae27b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_with_cloudflare(url: str, driver: webdriver) -> None:\n",
    "\n",
    "    # open the url with the driver\n",
    "    driver.get(url)\n",
    "\n",
    "    # check if human verification is needed\n",
    "    if \"Just a moment\" in driver.title:\n",
    "        print(\"[yellow]Cloudflare verification detected, waiting for it to complete...[/yellow]\")\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        # wait for the page to load\n",
    "        while \"Just a moment\" in driver.title:\n",
    "            time.sleep(1)\n",
    "\n",
    "    # check if the page has loaded properly\n",
    "    if driver.current_url == url:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "40480636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_search_results(soup: BeautifulSoup) -> list[dict]:\n",
    "\n",
    "    # prepare results list\n",
    "    results = []\n",
    "\n",
    "    # get all ul object of class \"SearchResultList\" and ensure there is only one\n",
    "    if len((ul_objects := soup.find_all('ul', class_='SearchResultsList'))) != 1:\n",
    "        print(\"[red]Error: Expected exactly one SearchResultList, found {}.[/red]\".format(len(ul_objects)))\n",
    "\n",
    "    # get every li item from the ul object\n",
    "    li_items = ul_objects[0].find_all('li')\n",
    "\n",
    "    # iterate over the li_items\n",
    "    for li in li_items:\n",
    "        # prepare the result item\n",
    "        result_dict = {}\n",
    "\n",
    "        # get the card url\n",
    "        a_tag = li.find('a', class_='SearchStudyCard')\n",
    "        url = a_tag['href'] if a_tag else None\n",
    "        result_dict['url-sp'] = str(url).strip()\n",
    "\n",
    "        # get degree information\n",
    "        header_text = li.find('h2', class_='StudyName')\n",
    "        result_dict['degree'] = str(header_text.text).strip() if header_text else None\n",
    "\n",
    "        # get university information\n",
    "        org_name = li.find('strong', class_='OrganisationName')\n",
    "        org_location = li.find('strong', class_='OrganisationLocation')\n",
    "        result_dict['university'] = str(org_name.text).strip() if org_name else None\n",
    "        result_dict['location'] = str(org_location.text).strip() if org_location else None\n",
    "\n",
    "        # append the result dict if not all None values\n",
    "        if all(r is not None for r in result_dict.values()):\n",
    "            # add uuid to the result dict\n",
    "            result_dict['uuid'] = str(uuid.uuid4())\n",
    "            results.append(result_dict)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3ba7edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_programs(driver: webdriver, entry_url: str) -> list[dict]:\n",
    "\n",
    "    # open a webdriver and navigate to entry url\n",
    "    open_with_cloudflare(entry_url, driver)\n",
    "\n",
    "    # first find how many pages exist\n",
    "    nav_tag = driver.find_element(\"css selector\", \"nav.PagNavigationContainer\")\n",
    "    see_more_label = nav_tag.find_element(\"css selector\", \"p.SeeMoreLabelVar1\")\n",
    "    page_count_text = see_more_label.text.split(' of ')\n",
    "\n",
    "    # ensure only two elements are in page_count_text\n",
    "    if len(page_count_text) != 2:\n",
    "        print(f\"[red]Error: Expected two elements in page_count_text, found {len(page_count_text)}.[/red]\")\n",
    "        driver.quit()\n",
    "\n",
    "    # get the total number of pages\n",
    "    total_pages = int(page_count_text[1].strip())\n",
    "    print(f\"[green]Total pages found: {total_pages}[/green]\")\n",
    "\n",
    "    # iterate over the pages\n",
    "    results = []\n",
    "    for page in tqdm(range(1, total_pages + 1), desc=\"Scraping programs\"):\n",
    "        # scrape the page\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        page_results = parse_search_results(soup)\n",
    "        results.extend(page_results)\n",
    "\n",
    "        # navigate to the next page\n",
    "        if page < total_pages:\n",
    "            nav_tag = driver.find_element(\"css selector\", \"nav.PagNavigationContainer\")\n",
    "            next_page = nav_tag.find_element(\"css selector\", 'a[title=\"Next page\"]')\n",
    "            next_page.click()\n",
    "            time.sleep(5 + (5*random.random()))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d25d9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attempt_load_all(file: PathLike, entry_url: str = None, delimiter: str = ';') -> pd.DataFrame:\n",
    "\n",
    "    # generate file if it does not exist\n",
    "    if not (file := Path(file)).exists():\n",
    "        print(f\"[yellow]File {file} does not exist, creating a new DataFrame.[/yellow]\")\n",
    "\n",
    "        # check that entry_url exists\n",
    "        if entry_url is None:\n",
    "            raise ValueError(\"entry_url must be provided if the file does not exist\")\n",
    "\n",
    "        # attempt to scrape for programs database\n",
    "        driver = uc.Chrome(use_subprocess=True, headless=False)\n",
    "        results = get_all_programs(driver, entry_url)\n",
    "        driver.quit()\n",
    "\n",
    "        # save the results to a csv file\n",
    "        with file.open('w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(\n",
    "                csvfile,\n",
    "                fieldnames=list(results[0].keys()),\n",
    "                delimiter=delimiter,\n",
    "            )\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results)\n",
    "\n",
    "    # attempt to load\n",
    "    try:\n",
    "        # print(f\"[green]Loaded existing data from {file}[/green]\")\n",
    "        return pd.read_csv(file, delimiter=delimiter)\n",
    "    except Exception as e:\n",
    "        print(f\"[red]Error loading CSV: {e}[/red]\")\n",
    "        raise e\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77898901",
   "metadata": {},
   "source": [
    "## Program Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f83de530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_by_string(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    key: Iterable[str],\n",
    "    adjust: int,\n",
    "    exact_match: bool = False,\n",
    "    only_one: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    # check that col in df\n",
    "    if col not in df.columns:\n",
    "        return df\n",
    "\n",
    "    # check if key is a single object, such as a string\n",
    "    if isinstance(key, str):\n",
    "        key = [key]\n",
    "    elif not isinstance(key, Iterable):\n",
    "        raise TypeError(\"key must be a string or an iterable of strings\")\n",
    "\n",
    "    # adjust score by containment or exact match\n",
    "    starting_score = df['score'].copy()\n",
    "    for k in key:\n",
    "        if exact_match:\n",
    "            df.loc[df[col] == k, 'score'] += adjust\n",
    "        else:\n",
    "            df.loc[df[col].str.contains(k, case=False, na=False), 'score'] += adjust\n",
    "\n",
    "    # if only_one, ensure all scores are within adjust of the starting score\n",
    "    if only_one:\n",
    "        df.loc[df['score'] > starting_score + adjust, 'score'] = starting_score + adjust\n",
    "        df.loc[df['score'] < starting_score - adjust, 'score'] = starting_score - adjust\n",
    "\n",
    "    # sort by score descending\n",
    "    df.sort_values(by='score', ascending=False, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0a4fb8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe_by_col(df: pd.DataFrame, group_col: str, compare_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Deduplicate DataFrame by group_col, keeping the row with the highest score for each group.\n",
    "    \"\"\"\n",
    "    if group_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{group_col}' not found in DataFrame\")\n",
    "    if compare_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{compare_col}' not found in DataFrame\")\n",
    "\n",
    "    # remove duplicates in group_col and keep the highest compare_col\n",
    "    df = df.loc[df.groupby(group_col)[compare_col].idxmax()]\n",
    "\n",
    "    # sort by compare_col\n",
    "    df.sort_values(by=compare_col, ascending=False, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b7a6fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_score_masters(df: pd.DataFrame, reset: bool = True, score_cut: int = 0, dedupe: bool = True) -> pd.DataFrame:\n",
    "\n",
    "    # reset scoring\n",
    "    if reset:\n",
    "        df['score'] = 0\n",
    "\n",
    "    # disqualifying\n",
    "    df = score_by_string(\n",
    "        df=df,\n",
    "        col='degree',\n",
    "        key=[\n",
    "            'Appl', 'Engineering', 'Life', 'Photonics', 'Bio', 'Modelling', 'Material',\n",
    "            'Molecular', 'Atmos', 'Medical', 'Solid', 'Catalys', 'Comp', 'Nano', 'Simulation',\n",
    "            'Communication', 'Education', 'Climate', 'Neuro', 'Tech', 'Data Science',\n",
    "            'Architecture', 'Information', 'Computer', 'Sustainability', 'Business', 'Manage',\n",
    "            'Optics', 'Systems', 'Instrument', 'Science', 'Geo', 'Chemistry', 'Env',\n",
    "            'Experimental',\n",
    "        ],\n",
    "        adjust=-10,\n",
    "    )\n",
    "\n",
    "    df = score_by_string(\n",
    "        df=df,\n",
    "        col='location',\n",
    "        key=[\n",
    "            'Singapore', 'South Africa', 'Malaysia', 'India', 'Pakistan', 'United Arab Emirates',\n",
    "            'Hungary', 'Russia', 'Israel', 'Egypt', 'Saudi Arabia', 'Turkey', 'Oman', 'Thailand',\n",
    "            'United States', 'Australia', 'Kazakhstan', 'Multiple locations', 'China'\n",
    "        ],\n",
    "        adjust=-10,\n",
    "    )\n",
    "\n",
    "    # non-prefer\n",
    "    df = score_by_string(\n",
    "        df=df,\n",
    "        col='location',\n",
    "        key=[\n",
    "            'Spain', 'Italy', 'United Kingdom', 'Finland', 'Bulgaria', 'Kenya', 'Uganda', 'Taiwan',\n",
    "            'Poland', 'Namibia', 'Nepal', 'Iceland'\n",
    "        ],\n",
    "        adjust=-1,\n",
    "    )\n",
    "\n",
    "    # prefer\n",
    "    df = score_by_string(\n",
    "        df=df,\n",
    "        col='degree',\n",
    "        key='Physics',\n",
    "        adjust=1,\n",
    "        exact_match=True,\n",
    "    )\n",
    "\n",
    "    df = score_by_string(\n",
    "        df=df,\n",
    "        col='degree',\n",
    "        key=[\n",
    "            'theor', 'math'\n",
    "        ],\n",
    "        adjust=4,\n",
    "        only_one=True,\n",
    "    )\n",
    "\n",
    "    df = score_by_string(\n",
    "        df=df,\n",
    "        col='degree',\n",
    "        key=[\n",
    "            'particle', 'nuclear', 'grav', 'cosmology'\n",
    "        ],\n",
    "        adjust=3,\n",
    "        only_one=True,\n",
    "    )\n",
    "\n",
    "    df = score_by_string(\n",
    "        df=df,\n",
    "        col='location',\n",
    "        key=[\n",
    "            'Germany', 'Denmark', 'Netherlands', 'Estonia', 'New Zealand', 'Austria'\n",
    "        ],\n",
    "        adjust=1,\n",
    "    )\n",
    "\n",
    "    # eliminate programs with scores <= 0\n",
    "    if score_cut is not None:\n",
    "        df = df[df['score'] > score_cut]\n",
    "\n",
    "    # de-duplicate entries\n",
    "    if dedupe:\n",
    "        # if there are multiple entries with the same 'university'\n",
    "        # then keep only the row with the highest 'score'\n",
    "        df = dedupe_by_col(df, 'university', 'score')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cc0ea5",
   "metadata": {},
   "source": [
    "## Research Vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fc9aaa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_filename(name: str) -> str:\n",
    "\n",
    "    return name.replace(' ', '_'\n",
    "                ).replace('\\'', ''\n",
    "                ).replace('.', '_'\n",
    "                ).replace('_-_', '_'\n",
    "                ).replace('-', '_'\n",
    "                ).replace('(', '_'\n",
    "                ).replace(')', '_'\n",
    "                ).replace('__', '_'\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "253ccec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml(file: PathLike) -> dict:\n",
    "\n",
    "    # check that file exists\n",
    "    if not (file := Path(file)).exists():\n",
    "        raise FileNotFoundError(f\"File '{file}' does not exist\")\n",
    "\n",
    "    # read the yaml file\n",
    "    with file.open('r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    if not content:\n",
    "        raise ValueError(f\"File '{file}' is empty\")\n",
    "\n",
    "    # parse the yaml content\n",
    "    if len(yaml_str := content.split('---')) < 2:\n",
    "        raise ValueError(f\"File '{file}' does not contain valid YAML header\")\n",
    "\n",
    "    if not (yaml_str := yaml_str[1].strip()):\n",
    "        raise ValueError(f\"File '{file}' does not contain valid YAML content\")\n",
    "\n",
    "    return yaml.safe_load(yaml_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "02a25068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vault(\n",
    "    df: pd.DataFrame,\n",
    "    dir: PathLike,\n",
    "    doc_prompts: dict[str, str] = None,\n",
    "    overwrite: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "    # check that vault_dir exists\n",
    "    dir = Path(dir)\n",
    "    if not dir.exists():\n",
    "        raise FileNotFoundError(f\"Vault directory '{dir}' does not exist\")\n",
    "\n",
    "    # add research task keys to masters_df\n",
    "    for key in research_tasks.keys():\n",
    "        if key not in df.columns:\n",
    "            df[key] = ''\n",
    "\n",
    "    # iterate over rows of the df to create research files\n",
    "    for _, row in df.iterrows():\n",
    "        filename = name_to_filename(row['university'].strip().lower()) + '.md'\n",
    "        if (not (filepath := (dir / filename)).exists()) or overwrite:\n",
    "            with filepath.open('w', encoding='utf-8') as f:\n",
    "                yaml_str = '---\\n' + '\\n'.join([f\"{k}: {v}\" for k, v in row.to_dict().items()]) + '\\n---'\n",
    "                f.write(yaml_str)\n",
    "                if doc_prompts:\n",
    "                    for doc_title, doc_prompt in doc_prompts.items():\n",
    "                        f.write(f\"\\n# {doc_title}\\n{doc_prompt}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d58401",
   "metadata": {},
   "source": [
    "# Masters Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bd3d03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to load from obsidian dir vault\n",
    "if not research_dir.exists():\n",
    "    # attempt to load the scored programs file\n",
    "    if not (scored_masters_file := csv_dir / 'masters_scored.csv').exists():\n",
    "        # attempt to load the full program file\n",
    "        masters_df = attempt_load_all(\n",
    "            file=(all_masters_file := csv_dir / 'masters_all.csv'),\n",
    "            entry_url=masters_entry_url,\n",
    "        )\n",
    "\n",
    "        # score, cut, and save\n",
    "        masters_df = first_score_masters(\n",
    "            df=masters_df,\n",
    "            reset=True,\n",
    "            score_cut=0,\n",
    "            dedupe=True,\n",
    "        )\n",
    "\n",
    "        masters_df.to_csv(\n",
    "            scored_masters_file,\n",
    "            index=False,\n",
    "            encoding='utf-8',\n",
    "            sep=';',\n",
    "        )\n",
    "\n",
    "    # read, sort, and re-index\n",
    "    masters_df = pd.read_csv(scored_masters_file, delimiter=';')\n",
    "\n",
    "    # update columns for user input\n",
    "    for key in research_tasks.keys():\n",
    "        if key not in masters_df.columns:\n",
    "            masters_df[key] = ''\n",
    "\n",
    "    # masters_df['completed'] = False\n",
    "    # masters_df['veto'] = False\n",
    "    # masters_df['score'] = 0\n",
    "    masters_df['icon'] = 'LiUniversity'\n",
    "    # masters_df['started'] = 'False'\n",
    "\n",
    "    # create the research vault for further user input\n",
    "    research_dir.mkdir(exist_ok=True, parents=False)\n",
    "    create_vault(\n",
    "        df=masters_df,\n",
    "        dir=research_dir,\n",
    "        doc_prompts=document_prompts,\n",
    "    )\n",
    "# else:\n",
    "#     # iterate over each file, construct dict from yaml header, turn to df, sort by score\n",
    "#     dict_list = []\n",
    "#     for f in research_dir.glob('*.md'):\n",
    "#         f_dict = read_yaml(f)\n",
    "#         f_dict['veto'] = bool(f_dict['veto'])\n",
    "#         f_dict['completed'] = bool(f_dict['completed'])\n",
    "#         dict_list.append(f_dict)\n",
    "\n",
    "#     # convert to masters_df and cut\n",
    "#     masters_df = pd.DataFrame(dict_list)\n",
    "#     masters_df = masters_df[(masters_df['completed']) & (~masters_df['veto'])]\n",
    "\n",
    "# sort and re-index\n",
    "# masters_df.sort_values(by='score', ascending=False, inplace=True)\n",
    "# masters_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5ae18d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masters_df[[\n",
    "#     'degree', 'university', 'location', 'research-topic', 'score',\n",
    "# ]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
